# ğŸš— Carzone.ie Web Crawler

> Crawler professionnel TypeScript/Node.js avec architecture modulaire et scalable pour extraire et sauvegarder les pages liste du site [carzone.ie](https://www.carzone.ie)

[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue.svg)](https://www.typescriptlang.org/)
[![Node.js](https://img.shields.io/badge/Node.js-20+-green.svg)](https://nodejs.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#-vue-densemble)
- [Architecture](#-architecture)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Configuration](#-configuration)
- [Structure du projet](#-structure-du-projet)
- [DÃ©veloppement](#-dÃ©veloppement)

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente un crawler web professionnel avec une architecture modulaire et scalable:

- âœ… **SÃ©paration claire des responsabilitÃ©s** (config, http, crawler, storage)
- âœ… **Gestion automatique du proxy** avec retry et timeout
- âœ… **Queue contrÃ´lÃ©e** pour Ã©viter les doublons
- âœ… **Retry automatique** avec backoff exponentiel
- âœ… **Rate limiting** pour respecter le site cible
- âœ… **Docker propre et reproductible**

## ğŸ—ï¸ Architecture

```
carzone-crawler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ env.ts          # Configuration centralisÃ©e
â”‚   â”œâ”€â”€ http/
â”‚   â”‚   â””â”€â”€ client.ts       # Client HTTP avec proxy + retry
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ queue.ts        # Gestion de la queue d'URLs
â”‚   â”‚   â”œâ”€â”€ listCrawler.ts  # Extraction des pages liste
â”‚   â”‚   â””â”€â”€ pageCrawler.ts  # Crawling des pages individuelles
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ fileStore.ts    # Sauvegarde des fichiers HTML
â”‚   â””â”€â”€ index.ts            # Point d'entrÃ©e et orchestration
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ Dockerfile
```

### Principes de conception

1. **SÃ©paration des responsabilitÃ©s**: Chaque module a une responsabilitÃ© unique et bien dÃ©finie
2. **Configuration centralisÃ©e**: Toute la config dans `config/env.ts`
3. **Gestion d'erreurs robuste**: Retry automatique avec axios-retry
4. **Queue contrÃ´lÃ©e**: Ã‰vite les doublons et gÃ¨re efficacement les URLs
5. **Respect du site**: Rate limiting avec dÃ©lais configurables

## âœ¨ FonctionnalitÃ©s

- âœ… **Architecture modulaire**: Code organisÃ© et maintenable
- âœ… **Retry automatique**: Backoff exponentiel pour les erreurs rÃ©seau
- âœ… **Proxy dynamique**: Configuration via variables d'environnement
- âœ… **Queue intelligente**: Gestion des URLs visitÃ©es avec Set (O(1))
- âœ… **Rate limiting**: DÃ©lais configurables entre requÃªtes
- âœ… **TypeScript strict**: Code type-safe avec types stricts
- âœ… **Docker optimisÃ©**: Image Alpine lÃ©gÃ¨re et reproductible
- âœ… **Logging structurÃ©**: Logs clairs avec prÃ©fixes de module

## ğŸš€ Installation

### PrÃ©requis

- **Docker** 20.10+ (recommandÃ©)
- **Node.js** 20+ et **npm** 9+ (pour dÃ©veloppement local)

### Construction de l'image Docker

```bash
# Construction standard
docker build -t carzone-crawler .

# Avec proxy (optionnel)
docker build \
  --build-arg PROXY_URL=http://proxy.example.com:3128 \
  -t carzone-crawler .
```

## ğŸ’» Utilisation

### ExÃ©cution avec Docker (RecommandÃ©)

#### Ã‰tape 1: Construire l'image Docker

**Linux/Mac:**

```bash
docker build -t carzone-crawler .
```

**Windows PowerShell:**

```powershell
docker build -t carzone-crawler .
```

**Avec proxy (optionnel lors du build):**

```bash
docker build --build-arg PROXY_URL=http://proxy.example.com:3128 -t carzone-crawler .
```

#### Ã‰tape 2: CrÃ©er le rÃ©pertoire de sortie

**Linux/Mac:**

```bash
mkdir -p output
```

**Windows PowerShell:**

```powershell
New-Item -ItemType Directory -Force -Path output
```

#### Ã‰tape 3: ExÃ©cuter le conteneur

**Linux/Mac - ExÃ©cution standard:**

```bash
docker run --rm -v "$(pwd)/output:/data" carzone-crawler
```

**Windows PowerShell - ExÃ©cution standard:**

```powershell
docker run --rm -v "${PWD}/output:/data" carzone-crawler
```

**Avec proxy:**

```bash
# Linux/Mac
docker run --rm \
  -e PROXY_URL=http://proxy.example.com:3128 \
  -v "$(pwd)/output:/data" \
  carzone-crawler
```

```powershell
# Windows PowerShell
docker run --rm `
  -e PROXY_URL=http://proxy.example.com:3128 `
  -v "${PWD}/output:/data" `
  carzone-crawler
```

**Avec variables d'environnement personnalisÃ©es:**

```bash
# Linux/Mac
docker run --rm \
  -e OUTPUT_DIR=/data \
  -e DELAY_MS=2000 \
  -e MAX_PAGES=100 \
  -e HTTP_TIMEOUT=20000 \
  -e RETRIES=5 \
  -v "$(pwd)/output:/data" \
  carzone-crawler
```

```powershell
# Windows PowerShell
docker run --rm `
  -e OUTPUT_DIR=/data `
  -e DELAY_MS=2000 `
  -e MAX_PAGES=100 `
  -e HTTP_TIMEOUT=20000 `
  -e RETRIES=5 `
  -v "${PWD}/output:/data" `
  carzone-crawler
```

#### Commandes Docker utiles

```bash
# Voir les images Docker disponibles
docker images

# Voir les conteneurs en cours d'exÃ©cution
docker ps

# Voir tous les conteneurs (y compris arrÃªtÃ©s)
docker ps -a

# Voir les logs d'un conteneur (remplacez <container-id> par l'ID rÃ©el)
docker logs <container-id>

# Supprimer l'image Docker
docker rmi carzone-crawler

# Nettoyer les conteneurs arrÃªtÃ©s
docker container prune

# Nettoyer les images non utilisÃ©es
docker image prune
```

#### VÃ©rification de l'exÃ©cution

AprÃ¨s l'exÃ©cution, vÃ©rifiez que les fichiers ont Ã©tÃ© crÃ©Ã©s :

```bash
# Linux/Mac
ls -lh output/
head -n 50 output/page-001.html

# Windows PowerShell
Get-ChildItem output
Get-Content output/page-001.html -Head 50
```

#### DÃ©pannage Docker

**ProblÃ¨me: Docker Desktop n'est pas dÃ©marrÃ©**

```bash
# VÃ©rifier si Docker fonctionne
docker ps

# Si erreur: dÃ©marrer Docker Desktop et attendre qu'il soit complÃ¨tement dÃ©marrÃ©
```

**ProblÃ¨me: L'image n'existe pas**

```bash
# Reconstruire l'image
docker build -t carzone-crawler .
```

**ProblÃ¨me: Erreurs de permissions sur le rÃ©pertoire output**

```bash
# Linux/Mac: Ajuster les permissions
chmod 755 output
chown -R $USER:$USER output

# Windows: VÃ©rifier que le dossier existe et est accessible
```

**ProblÃ¨me: Le conteneur s'arrÃªte immÃ©diatement**

```bash
# VÃ©rifier les logs du dernier conteneur
docker logs $(docker ps -lq)

# ExÃ©cuter en mode interactif pour dÃ©boguer
docker run -it --rm -v "$(pwd)/output:/data" carzone-crawler sh
```

### ExÃ©cution locale (DÃ©veloppement)

```bash
# Installation des dÃ©pendances
npm install

# Compilation TypeScript
npm run build

# ExÃ©cution
npm start

# Ou en mode dÃ©veloppement
npm run dev
```

## âš™ï¸ Configuration

### Variables d'environnement

| Variable       | Description                         | DÃ©faut    |
| -------------- | ----------------------------------- | --------- |
| `PROXY_URL`    | URL du proxy (peut inclure auth)    | `""`      |
| `OUTPUT_DIR`   | RÃ©pertoire de sortie                | `"/data"` |
| `DELAY_MS`     | DÃ©lai entre requÃªtes (ms)           | `"1000"`  |
| `HTTP_TIMEOUT` | Timeout HTTP (ms)                   | `"15000"` |
| `RETRIES`      | Nombre de tentatives en cas d'Ã©chec | `"3"`     |

### Exemples de configuration

```bash
# Proxy avec authentification
PROXY_URL=http://user:pass@proxy.example.com:3128

# DÃ©lai plus long pour respecter le site
DELAY_MS=2000

# Plus de tentatives pour les connexions instables
RETRIES=5
```

### Fichier .env (DÃ©veloppement local)

Pour la configuration locale (sans Docker), vous pouvez crÃ©er un fichier `.env` Ã  la racine du projet :

```bash
# Copier le fichier d'exemple
cp .env.example .env

# Ã‰diter avec vos valeurs
nano .env  # ou votre Ã©diteur prÃ©fÃ©rÃ©
```

**Exemple de fichier `.env` :**

```env
PROXY_URL=http://proxy.example.com:3128
OUTPUT_DIR=./output
DELAY_MS=1000
HTTP_TIMEOUT=15000
RETRIES=3
```

âš ï¸ **Important**: Ne commitez jamais le fichier `.env` avec des credentials rÃ©els. Le fichier `.env.example` sert de template.

**Note**: Pour charger automatiquement les variables depuis `.env` en dÃ©veloppement local, vous pouvez installer `dotenv` :

```bash
npm install dotenv
```

Puis ajoutez au dÃ©but de `src/index.ts` :

```typescript
import "dotenv/config";
```

En production Docker, utilisez les variables d'environnement directement via `-e` ou un fichier `.env` montÃ©.

## ğŸ“ Structure du projet

```
DATABIZ_EXERCICE/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ env.ts              # Configuration centralisÃ©e
â”‚   â”‚
â”‚   â”œâ”€â”€ http/
â”‚   â”‚   â””â”€â”€ client.ts           # Client HTTP avec retry
â”‚   â”‚
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ queue.ts            # Queue de gestion des URLs
â”‚   â”‚   â”œâ”€â”€ listCrawler.ts      # Extraction des URLs liste
â”‚   â”‚   â””â”€â”€ pageCrawler.ts      # Crawling des pages
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ fileStore.ts        # Sauvegarde des fichiers
â”‚   â”‚
â”‚   â””â”€â”€ index.ts                # Point d'entrÃ©e principal
â”‚
â”œâ”€â”€ dist/                       # Code compilÃ© (gÃ©nÃ©rÃ©)
â”œâ”€â”€ output/                     # Fichiers HTML (gÃ©nÃ©rÃ©)
â”‚
â”œâ”€â”€ Dockerfile                  # Configuration Docker
â”œâ”€â”€ package.json                # DÃ©pendances npm
â”œâ”€â”€ tsconfig.json               # Configuration TypeScript
â”œâ”€â”€ .env.example                # Template de configuration
â”‚
â”œâ”€â”€ README.md                   # Ce fichier
```

## ğŸ”§ DÃ©veloppement

### Scripts disponibles

```bash
npm run build    # Compiler TypeScript
npm start        # ExÃ©cuter le code compilÃ©
npm run dev      # ExÃ©cuter avec ts-node (dev)
npm run clean    # Nettoyer les fichiers gÃ©nÃ©rÃ©s
```

### Format de sortie

Les pages HTML sont sauvegardÃ©es avec des noms sÃ©quentiels:

```
output/
â”œâ”€â”€ page-001.html
â”œâ”€â”€ page-002.html
â”œâ”€â”€ page-003.html
â””â”€â”€ ...
```

### Logs

Le crawler affiche des logs structurÃ©s:

```
============================================================
ğŸš€ CARZONE.IE CRAWLER
============================================================
ğŸ“ Base URL: https://www.carzone.ie
ğŸ“„ Max Pages: 200
ğŸ’¾ Output Dir: /data
â±ï¸  Delay: 1000ms
ğŸ”„ Retries: 3
============================================================

[STEP 1] Extracting listing URLs...
[LIST] Fetching listing page: https://www.carzone.ie/cars
[LIST] Found 45 listing URLs

[STEP 2] Starting page crawling...
[PAGE] Crawling 1: https://www.carzone.ie/cars
[STORAGE] File saved: /data/page-001.html
[PAGE] Saved: page-001.html
...
```

## ğŸ¯ Principes de conception

### 1. SÃ©paration des responsabilitÃ©s

- **config/**: Configuration centralisÃ©e
- **http/**: Client HTTP avec retry
- **crawler/**: Logique de crawling
- **storage/**: Gestion du stockage

### 2. Gestion automatique du proxy

Le proxy est configurÃ© automatiquement depuis les variables d'environnement, avec support de l'authentification.

### 3. Queue contrÃ´lÃ©e

La classe `UrlQueue` gÃ¨re:

- Les URLs visitÃ©es (Set pour O(1) lookup)
- La file d'attente (FIFO)
- La normalisation des URLs

### 4. Retry + Timeout

- **Retry automatique**: 3 tentatives par dÃ©faut avec backoff exponentiel
- **Timeout**: 15 secondes par dÃ©faut
- **Gestion d'erreurs**: Erreurs rÃ©seau et 5xx retentÃ©es automatiquement

### 5. Respect du site

- **Rate limiting**: DÃ©lai configurable entre requÃªtes (1s par dÃ©faut)
- **User-Agent rÃ©aliste**: Ã‰vite la dÃ©tection de bot
- **Headers appropriÃ©s**: Headers HTTP rÃ©alistes

### 6. Docker propre

- **Image Alpine**: LÃ©gÃ¨re et sÃ©curisÃ©e
- **Multi-stage optimisÃ©**: Cache Docker optimisÃ©
- **Variables d'environnement**: Configuration flexible

## ğŸ“ Notes importantes

- âš ï¸ **Respectez les conditions d'utilisation** du site carzone.ie
- âš ï¸ **Respectez le robots.txt** et les directives du site
- âš ï¸ **Utilisez des dÃ©lais appropriÃ©s** pour ne pas surcharger le serveur
- âš ï¸ **Ce crawler est Ã  des fins Ã©ducatives/d'exercice uniquement**

## ğŸ“„ Licence

ISC License

## ğŸ‘¤ Auteur

**Samer Smati**

---

**Bon crawling! ğŸš€**
